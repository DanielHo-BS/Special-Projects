{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#等距抽样\n",
    "def dataSample(data,sample_count):\n",
    "    record_count=data.shape[0]\n",
    "    width=int(record_count/sample_count)#计算抽样间距\n",
    "    data_sample=[]\n",
    "    n=0\n",
    "    for i in range(record_count-1):\n",
    "        if i%width==0:\n",
    "            data_sample.append(data.iloc[[i]])\n",
    "            n +=1\n",
    "        if  n==sample_count:\n",
    "            break\n",
    "        \n",
    "    data_sample = np.array(data_sample)\n",
    "    return data_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfcombine(dfh,dfp,df,label,sample_count):\n",
    "    df=df\n",
    "    label=label\n",
    "    dfh=pd.read_csv(dfh)\n",
    "    dfp=pd.read_csv(dfp)\n",
    "\n",
    "    print('orginal data')\n",
    "    print(dfh.shape)\n",
    "    print(dfp.shape)\n",
    "\n",
    "    dfh=dfh.drop(labels=['frame'],axis=1)\n",
    "    dfh=dataSample(dfh,sample_count)\n",
    "    dfp=dfp.drop(labels=['frame'],axis=1)\n",
    "    dfp=dataSample(dfp,sample_count)\n",
    "\n",
    "    print('smaple data')\n",
    "    print(dfh.shape)\n",
    "    print(dfp.shape)\n",
    "\n",
    "\n",
    "\n",
    "    labelH=[]\n",
    "    labelP=[]\n",
    "    for i in range(sample_count):\n",
    "        labelH=np.append(labelH,[0],axis=0)\n",
    "        labelP=np.append(labelP,[1],axis=0)\n",
    "\n",
    "    print('label')\n",
    "    print(labelH.shape)\n",
    "    print(labelP.shape)\n",
    "    print('\\n')\n",
    "\n",
    "    df.append([dfh])\n",
    "    df.append([dfp])\n",
    "    label.append([labelH])\n",
    "    label.append([labelP])\n",
    "\n",
    "    return df,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orginal data\n",
      "(5503, 9)\n",
      "(4761, 9)\n",
      "smaple data\n",
      "(250, 1, 8)\n",
      "(250, 1, 8)\n",
      "label\n",
      "(250,)\n",
      "(250,)\n",
      "\n",
      "\n",
      "orginal data\n",
      "(1457, 9)\n",
      "(7240, 9)\n",
      "smaple data\n",
      "(250, 1, 8)\n",
      "(250, 1, 8)\n",
      "label\n",
      "(250,)\n",
      "(250,)\n",
      "\n",
      "\n",
      "orginal data\n",
      "(7338, 9)\n",
      "(8733, 9)\n",
      "smaple data\n",
      "(250, 1, 8)\n",
      "(250, 1, 8)\n",
      "label\n",
      "(250,)\n",
      "(250,)\n",
      "\n",
      "\n",
      "orginal data\n",
      "(7555, 9)\n",
      "(7872, 9)\n",
      "smaple data\n",
      "(250, 1, 8)\n",
      "(250, 1, 8)\n",
      "label\n",
      "(250,)\n",
      "(250,)\n",
      "\n",
      "\n",
      "orginal data\n",
      "(4117, 9)\n",
      "(7653, 9)\n",
      "smaple data\n",
      "(250, 1, 8)\n",
      "(250, 1, 8)\n",
      "label\n",
      "(250,)\n",
      "(250,)\n",
      "\n",
      "\n",
      "orginal data\n",
      "(5219, 9)\n",
      "(4963, 9)\n",
      "smaple data\n",
      "(250, 1, 8)\n",
      "(250, 1, 8)\n",
      "label\n",
      "(250,)\n",
      "(250,)\n",
      "\n",
      "\n",
      "(3000, 8)\n",
      "(3000,)\n"
     ]
    }
   ],
   "source": [
    "df=[]\n",
    "label=[]\n",
    "df,label=dfcombine(r'B/DistanceHealth.csv',r'B/DistancePain.csv',df,label,250)\n",
    "df,label=dfcombine(r'KT/DistanceHealth.csv',r'KT/DistancePain.csv',df,label,250)\n",
    "df,label=dfcombine(r'MS/DistanceHealth.csv',r'MS/DistancePain.csv',df,label,250)\n",
    "df,label=dfcombine(r'SW_line/DistanceHealth.csv',r'SW_line/DistancePain.csv',df,label,250)\n",
    "df,label=dfcombine(r'SW_smooth/DistanceHealth.csv',r'SW_smooth/DistancePain.csv',df,label,250)\n",
    "df,label=dfcombine(r'TW/DistanceHealth.csv',r'TW/DistancePain.csv',df,label,250)\n",
    "df=np.array(df)\n",
    "df=df.reshape(-1,8)\n",
    "print(df.shape)\n",
    "label=np.array(label)\n",
    "label=label.reshape(-1)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>painlevel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>222.991031</td>\n",
       "      <td>368.201032</td>\n",
       "      <td>589.020373</td>\n",
       "      <td>216.113396</td>\n",
       "      <td>337.965975</td>\n",
       "      <td>423.178449</td>\n",
       "      <td>171.230838</td>\n",
       "      <td>328.525494</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>215.573653</td>\n",
       "      <td>343.023323</td>\n",
       "      <td>502.802148</td>\n",
       "      <td>222.319590</td>\n",
       "      <td>424.571549</td>\n",
       "      <td>387.734445</td>\n",
       "      <td>153.447059</td>\n",
       "      <td>383.631333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>195.243438</td>\n",
       "      <td>342.720002</td>\n",
       "      <td>422.427509</td>\n",
       "      <td>238.907932</td>\n",
       "      <td>422.313864</td>\n",
       "      <td>329.946965</td>\n",
       "      <td>143.502613</td>\n",
       "      <td>334.598267</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>227.721321</td>\n",
       "      <td>428.590714</td>\n",
       "      <td>515.053395</td>\n",
       "      <td>240.010416</td>\n",
       "      <td>489.204456</td>\n",
       "      <td>436.202934</td>\n",
       "      <td>150.791246</td>\n",
       "      <td>286.876280</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>220.204450</td>\n",
       "      <td>283.001767</td>\n",
       "      <td>467.080293</td>\n",
       "      <td>208.892317</td>\n",
       "      <td>340.587727</td>\n",
       "      <td>309.919344</td>\n",
       "      <td>134.766465</td>\n",
       "      <td>359.402282</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>235.172277</td>\n",
       "      <td>430.056973</td>\n",
       "      <td>594.899151</td>\n",
       "      <td>258.799150</td>\n",
       "      <td>485.597570</td>\n",
       "      <td>457.389331</td>\n",
       "      <td>182.222392</td>\n",
       "      <td>416.004808</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>234.132441</td>\n",
       "      <td>325.161498</td>\n",
       "      <td>560.000893</td>\n",
       "      <td>207.386113</td>\n",
       "      <td>381.921458</td>\n",
       "      <td>400.216192</td>\n",
       "      <td>141.735669</td>\n",
       "      <td>413.696749</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>223.986607</td>\n",
       "      <td>359.824957</td>\n",
       "      <td>486.738123</td>\n",
       "      <td>248.394847</td>\n",
       "      <td>463.365946</td>\n",
       "      <td>377.809476</td>\n",
       "      <td>142.863571</td>\n",
       "      <td>400.719353</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>196.155551</td>\n",
       "      <td>347.139741</td>\n",
       "      <td>338.514401</td>\n",
       "      <td>233.548710</td>\n",
       "      <td>462.299686</td>\n",
       "      <td>305.006557</td>\n",
       "      <td>98.020406</td>\n",
       "      <td>293.001706</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>192.898937</td>\n",
       "      <td>379.795998</td>\n",
       "      <td>369.670123</td>\n",
       "      <td>253.885801</td>\n",
       "      <td>496.009072</td>\n",
       "      <td>327.954265</td>\n",
       "      <td>121.061968</td>\n",
       "      <td>339.524668</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4           5  \\\n",
       "0     222.991031  368.201032  589.020373  216.113396  337.965975  423.178449   \n",
       "1     215.573653  343.023323  502.802148  222.319590  424.571549  387.734445   \n",
       "2     195.243438  342.720002  422.427509  238.907932  422.313864  329.946965   \n",
       "3     227.721321  428.590714  515.053395  240.010416  489.204456  436.202934   \n",
       "4     220.204450  283.001767  467.080293  208.892317  340.587727  309.919344   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2995  235.172277  430.056973  594.899151  258.799150  485.597570  457.389331   \n",
       "2996  234.132441  325.161498  560.000893  207.386113  381.921458  400.216192   \n",
       "2997  223.986607  359.824957  486.738123  248.394847  463.365946  377.809476   \n",
       "2998  196.155551  347.139741  338.514401  233.548710  462.299686  305.006557   \n",
       "2999  192.898937  379.795998  369.670123  253.885801  496.009072  327.954265   \n",
       "\n",
       "               6           7  painlevel  \n",
       "0     171.230838  328.525494        0.0  \n",
       "1     153.447059  383.631333        0.0  \n",
       "2     143.502613  334.598267        0.0  \n",
       "3     150.791246  286.876280        0.0  \n",
       "4     134.766465  359.402282        0.0  \n",
       "...          ...         ...        ...  \n",
       "2995  182.222392  416.004808        1.0  \n",
       "2996  141.735669  413.696749        1.0  \n",
       "2997  142.863571  400.719353        1.0  \n",
       "2998   98.020406  293.001706        1.0  \n",
       "2999  121.061968  339.524668        1.0  \n",
       "\n",
       "[3000 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(df)\n",
    "df['painlevel'] =label\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326 159 178 337\n",
      "338 152 183 327\n",
      "319 166 168 347\n",
      "334 164 169 333\n",
      "323 182 159 336\n",
      "339 172 156 333\n",
      "301 178 200 321\n",
      "314 172 180 334\n",
      "331 187 161 321\n",
      "313 181 166 340\n",
      "[0.663, 0.665, 0.666, 0.667, 0.659, 0.672, 0.622, 0.648, 0.652, 0.653]\n",
      "0.6567000000000001\n"
     ]
    }
   ],
   "source": [
    "scores=[]\n",
    "data=df.drop(labels='painlevel',axis=1).values\n",
    "target=df.painlevel.values\n",
    "\n",
    "for j in range(10):\n",
    "\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=10/30, random_state=10*(j+1))\n",
    "\n",
    "    ss = StandardScaler()\n",
    "    X_train_Standar = ss.fit_transform(X_train)\n",
    "    X_test_Standar = ss.transform(X_test)\n",
    "\n",
    "    svc = svm.SVC(C=25, kernel='linear',probability=True)\n",
    "    svc.fit(X_train_Standar,y_train)\n",
    "    scores.append(svc.score(X_test_Standar, y_test))\n",
    "    predicted = svc.predict(X_test_Standar)\n",
    "    \n",
    "    tT = 0\n",
    "    tF = 0\n",
    "    fT = 0\n",
    "    fF = 0\n",
    "    for i in range(len(y_test)):\n",
    "        if (y_test[i] == 0)&(predicted[i] == 0):\n",
    "            tT += 1\n",
    "        elif (y_test[i] == 0)&(predicted[i] == 1):\n",
    "            tF += 1\n",
    "        elif (y_test[i] == 1)&(predicted[i] == 0):\n",
    "            fT += 1\n",
    "        elif (y_test[i] == 1)&(predicted[i] == 1):\n",
    "            fF += 1\n",
    "    print(tT,tF,fT,fF)\n",
    "        \n",
    "    ##save result\n",
    "    file = 'result_NO.csv'\n",
    "    with open(file, 'a+', newline = '') as csvFile:\n",
    "        csvWriter = csv.writer(csvFile)\n",
    "        csvWriter.writerow([tT,tF,fT,fF,'N'])    \n",
    "        \n",
    "print(scores)\n",
    "print(sum(scores)/len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Standar=ss.transform(df.drop(labels='painlevel',axis=1).values)\n",
    "pred = svc.predict(df_Standar)\n",
    "face = (pred==label)\n",
    "face = face*1\n",
    "face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfcombine2(sitaH,sitaP,df,sample_count):\n",
    "    df=df\n",
    "    dfh = pd.read_csv(sitaH)\n",
    "    dfp = pd.read_csv(sitaP)\n",
    "\n",
    "    print('orginal data')\n",
    "    print(dfh.shape)\n",
    "    print(dfp.shape)\n",
    "\n",
    "    dfh=dfh.drop(labels=['frame'],axis=1)\n",
    "    dfh=dataSample(dfh,sample_count)\n",
    "    dfp=dfp.drop(labels=['frame'],axis=1)\n",
    "    dfp=dataSample(dfp,sample_count)\n",
    "\n",
    "    print('smaple data')\n",
    "    print(dfh.shape)\n",
    "    print(dfp.shape)\n",
    "\n",
    "    df.append([dfh])\n",
    "    df.append([dfp])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orginal data\n",
      "(5503, 16)\n",
      "(4761, 16)\n",
      "smaple data\n",
      "(250, 1, 15)\n",
      "(250, 1, 15)\n",
      "orginal data\n",
      "(1457, 16)\n",
      "(7240, 16)\n",
      "smaple data\n",
      "(250, 1, 15)\n",
      "(250, 1, 15)\n",
      "orginal data\n",
      "(7338, 16)\n",
      "(8733, 16)\n",
      "smaple data\n",
      "(250, 1, 15)\n",
      "(250, 1, 15)\n",
      "orginal data\n",
      "(7555, 16)\n",
      "(7872, 16)\n",
      "smaple data\n",
      "(250, 1, 15)\n",
      "(250, 1, 15)\n",
      "orginal data\n",
      "(4117, 16)\n",
      "(7653, 16)\n",
      "smaple data\n",
      "(250, 1, 15)\n",
      "(250, 1, 15)\n",
      "orginal data\n",
      "(5219, 16)\n",
      "(4963, 16)\n",
      "smaple data\n",
      "(250, 1, 15)\n",
      "(250, 1, 15)\n",
      "df_sita\n",
      "(3000, 15)\n"
     ]
    }
   ],
   "source": [
    "df_sita=[]\n",
    "df_sita=dfcombine2(r'B/sitaHealth.csv',r'B/sitaPain.csv',df_sita,250)\n",
    "df_sita=dfcombine2(r'KT/sitaHealth.csv',r'KT/sitaPain.csv',df_sita,250)\n",
    "df_sita=dfcombine2(r'MS/sitaHealth.csv',r'MS/sitaPain.csv',df_sita,250)\n",
    "df_sita=dfcombine2(r'SW_line/sitaHealth.csv',r'SW_line/sitaPain.csv',df_sita,250)\n",
    "df_sita=dfcombine2(r'SW_smooth/sitaHealth.csv',r'SW_smooth/sitaPain.csv',df_sita,250)\n",
    "df_sita=dfcombine2(r'TW/sitaHealth.csv',r'TW/sitaPain.csv',df_sita,250)\n",
    "df_sita=np.array(df_sita)\n",
    "df_sita=df_sita.reshape(-1,15)\n",
    "print('df_sita')\n",
    "print(df_sita.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>face</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36.498051</td>\n",
       "      <td>79.149447</td>\n",
       "      <td>64.352502</td>\n",
       "      <td>17.171511</td>\n",
       "      <td>128.753684</td>\n",
       "      <td>34.074804</td>\n",
       "      <td>69.184814</td>\n",
       "      <td>64.945033</td>\n",
       "      <td>45.870153</td>\n",
       "      <td>30.277698</td>\n",
       "      <td>34.407984</td>\n",
       "      <td>115.314319</td>\n",
       "      <td>49.604237</td>\n",
       "      <td>51.579495</td>\n",
       "      <td>78.816268</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.312593</td>\n",
       "      <td>53.428331</td>\n",
       "      <td>96.259076</td>\n",
       "      <td>23.826775</td>\n",
       "      <td>109.571748</td>\n",
       "      <td>46.601478</td>\n",
       "      <td>67.145001</td>\n",
       "      <td>71.865210</td>\n",
       "      <td>40.989789</td>\n",
       "      <td>49.657599</td>\n",
       "      <td>42.961788</td>\n",
       "      <td>87.380613</td>\n",
       "      <td>56.143417</td>\n",
       "      <td>66.788563</td>\n",
       "      <td>57.068021</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27.102011</td>\n",
       "      <td>53.100917</td>\n",
       "      <td>99.797072</td>\n",
       "      <td>26.628476</td>\n",
       "      <td>104.133315</td>\n",
       "      <td>49.238209</td>\n",
       "      <td>54.774135</td>\n",
       "      <td>88.326979</td>\n",
       "      <td>36.898886</td>\n",
       "      <td>50.558863</td>\n",
       "      <td>52.281236</td>\n",
       "      <td>77.159901</td>\n",
       "      <td>51.032397</td>\n",
       "      <td>78.909712</td>\n",
       "      <td>50.057890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.736510</td>\n",
       "      <td>61.155655</td>\n",
       "      <td>91.107835</td>\n",
       "      <td>26.047712</td>\n",
       "      <td>96.691741</td>\n",
       "      <td>57.260547</td>\n",
       "      <td>66.824691</td>\n",
       "      <td>75.676833</td>\n",
       "      <td>37.498476</td>\n",
       "      <td>33.847288</td>\n",
       "      <td>56.317676</td>\n",
       "      <td>89.835036</td>\n",
       "      <td>35.536086</td>\n",
       "      <td>82.365388</td>\n",
       "      <td>62.098527</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40.033901</td>\n",
       "      <td>55.759028</td>\n",
       "      <td>84.207071</td>\n",
       "      <td>23.389639</td>\n",
       "      <td>122.643116</td>\n",
       "      <td>33.967245</td>\n",
       "      <td>76.328051</td>\n",
       "      <td>67.183203</td>\n",
       "      <td>36.488745</td>\n",
       "      <td>50.239826</td>\n",
       "      <td>37.251486</td>\n",
       "      <td>92.508688</td>\n",
       "      <td>66.884087</td>\n",
       "      <td>60.641126</td>\n",
       "      <td>52.474787</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>28.958591</td>\n",
       "      <td>62.302004</td>\n",
       "      <td>88.739405</td>\n",
       "      <td>21.073749</td>\n",
       "      <td>114.552929</td>\n",
       "      <td>44.373323</td>\n",
       "      <td>61.588898</td>\n",
       "      <td>75.448480</td>\n",
       "      <td>42.962622</td>\n",
       "      <td>44.366082</td>\n",
       "      <td>46.291099</td>\n",
       "      <td>89.342819</td>\n",
       "      <td>52.250925</td>\n",
       "      <td>67.364847</td>\n",
       "      <td>60.384228</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>37.602573</td>\n",
       "      <td>57.931588</td>\n",
       "      <td>84.465839</td>\n",
       "      <td>20.826234</td>\n",
       "      <td>121.747745</td>\n",
       "      <td>37.426021</td>\n",
       "      <td>81.903253</td>\n",
       "      <td>61.274757</td>\n",
       "      <td>36.821990</td>\n",
       "      <td>47.039818</td>\n",
       "      <td>35.114275</td>\n",
       "      <td>97.845908</td>\n",
       "      <td>63.816157</td>\n",
       "      <td>55.940508</td>\n",
       "      <td>60.243334</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>28.152172</td>\n",
       "      <td>49.283859</td>\n",
       "      <td>102.563969</td>\n",
       "      <td>26.381961</td>\n",
       "      <td>105.069846</td>\n",
       "      <td>48.548193</td>\n",
       "      <td>63.254433</td>\n",
       "      <td>82.024361</td>\n",
       "      <td>34.721206</td>\n",
       "      <td>54.015776</td>\n",
       "      <td>46.602122</td>\n",
       "      <td>79.382102</td>\n",
       "      <td>55.785987</td>\n",
       "      <td>72.984083</td>\n",
       "      <td>51.229931</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>22.862558</td>\n",
       "      <td>43.438348</td>\n",
       "      <td>113.699094</td>\n",
       "      <td>35.004568</td>\n",
       "      <td>81.873825</td>\n",
       "      <td>63.121607</td>\n",
       "      <td>55.889270</td>\n",
       "      <td>99.671019</td>\n",
       "      <td>24.439711</td>\n",
       "      <td>50.577487</td>\n",
       "      <td>66.236462</td>\n",
       "      <td>63.186051</td>\n",
       "      <td>38.435478</td>\n",
       "      <td>101.241030</td>\n",
       "      <td>40.323493</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>20.432521</td>\n",
       "      <td>43.420382</td>\n",
       "      <td>116.147097</td>\n",
       "      <td>31.381981</td>\n",
       "      <td>86.325816</td>\n",
       "      <td>62.292204</td>\n",
       "      <td>47.025084</td>\n",
       "      <td>105.640734</td>\n",
       "      <td>27.334181</td>\n",
       "      <td>53.854893</td>\n",
       "      <td>64.596240</td>\n",
       "      <td>61.548867</td>\n",
       "      <td>42.905433</td>\n",
       "      <td>95.978221</td>\n",
       "      <td>41.116346</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1           2          3           4          5  \\\n",
       "0     36.498051  79.149447   64.352502  17.171511  128.753684  34.074804   \n",
       "1     30.312593  53.428331   96.259076  23.826775  109.571748  46.601478   \n",
       "2     27.102011  53.100917   99.797072  26.628476  104.133315  49.238209   \n",
       "3     27.736510  61.155655   91.107835  26.047712   96.691741  57.260547   \n",
       "4     40.033901  55.759028   84.207071  23.389639  122.643116  33.967245   \n",
       "...         ...        ...         ...        ...         ...        ...   \n",
       "2995  28.958591  62.302004   88.739405  21.073749  114.552929  44.373323   \n",
       "2996  37.602573  57.931588   84.465839  20.826234  121.747745  37.426021   \n",
       "2997  28.152172  49.283859  102.563969  26.381961  105.069846  48.548193   \n",
       "2998  22.862558  43.438348  113.699094  35.004568   81.873825  63.121607   \n",
       "2999  20.432521  43.420382  116.147097  31.381981   86.325816  62.292204   \n",
       "\n",
       "              6           7          8          9         10          11  \\\n",
       "0     69.184814   64.945033  45.870153  30.277698  34.407984  115.314319   \n",
       "1     67.145001   71.865210  40.989789  49.657599  42.961788   87.380613   \n",
       "2     54.774135   88.326979  36.898886  50.558863  52.281236   77.159901   \n",
       "3     66.824691   75.676833  37.498476  33.847288  56.317676   89.835036   \n",
       "4     76.328051   67.183203  36.488745  50.239826  37.251486   92.508688   \n",
       "...         ...         ...        ...        ...        ...         ...   \n",
       "2995  61.588898   75.448480  42.962622  44.366082  46.291099   89.342819   \n",
       "2996  81.903253   61.274757  36.821990  47.039818  35.114275   97.845908   \n",
       "2997  63.254433   82.024361  34.721206  54.015776  46.602122   79.382102   \n",
       "2998  55.889270   99.671019  24.439711  50.577487  66.236462   63.186051   \n",
       "2999  47.025084  105.640734  27.334181  53.854893  64.596240   61.548867   \n",
       "\n",
       "             12          13         14  face  \n",
       "0     49.604237   51.579495  78.816268     1  \n",
       "1     56.143417   66.788563  57.068021     0  \n",
       "2     51.032397   78.909712  50.057890     0  \n",
       "3     35.536086   82.365388  62.098527     1  \n",
       "4     66.884087   60.641126  52.474787     1  \n",
       "...         ...         ...        ...   ...  \n",
       "2995  52.250925   67.364847  60.384228     1  \n",
       "2996  63.816157   55.940508  60.243334     1  \n",
       "2997  55.785987   72.984083  51.229931     1  \n",
       "2998  38.435478  101.241030  40.323493     0  \n",
       "2999  42.905433   95.978221  41.116346     0  \n",
       "\n",
       "[3000 rows x 16 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sita=pd.DataFrame(df_sita)\n",
    "df_sita['face'] =face\n",
    "df_sita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import time\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def SvmRfc(df_sita,FaceModel,SSModel,FaceModelSVM,FaceModelSVMLinear):   \n",
    "    df1 = df_sita[df_sita.face==0].reset_index(drop=True)\n",
    "    df2 = df_sita[df_sita.face==1].reset_index(drop=True)\n",
    "    df2 = pd.DataFrame.sample(df2,n=len(df1),random_state=1).reset_index(drop=True)\n",
    "    df3 = pd.concat([df1,df2]).reset_index(drop=True) \n",
    "    data = df3.drop(labels=['face'],axis=1).values\n",
    "    target = df3.face.values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "    ss = StandardScaler()\n",
    "    X_train = ss.fit_transform(X_train)\n",
    "    X_test = ss.transform(X_test)\n",
    "    \n",
    "    ##SVMLinear###########################\n",
    "    print(\"svm_linear\\n\")\n",
    "    svc_model_linear = svm.SVC(C=25, kernel='linear',probability=True)\n",
    "\n",
    "\n",
    "    svc_model_linear.fit(X_train, y_train)\n",
    "\n",
    "    print(\"score of train\",svc_model_linear.score(X_train,y_train))\n",
    "    \n",
    "    ##test of svm_linear\n",
    "    svc_linear_pred = svc_model_linear.predict(X_test)\n",
    "    print(\"score of test :\",svc_model_linear.score(X_test, y_test))\n",
    "    print(confusion_matrix(y_test,svc_linear_pred))\n",
    "    print(classification_report(y_test,svc_linear_pred))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    ##SVM###############################\n",
    "    print(\"svm\\n\")\n",
    "    svc_model = svm.SVC(gamma='auto', C=25, kernel='rbf',probability=True)\n",
    "\n",
    "\n",
    "    svc_model.fit(X_train, y_train)\n",
    "\n",
    "    print(\"score of train\",svc_model.score(X_train,y_train))\n",
    "\n",
    "    ##test of svm\n",
    "    svc_pred = svc_model.predict(X_test)\n",
    "    print(\"score of test :\",svc_model.score(X_test, y_test))\n",
    "    print(confusion_matrix(y_test,svc_pred))\n",
    "    print(classification_report(y_test,svc_pred))\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "    \n",
    "    ##RFC################################\n",
    "    print(\"RFC\\n\")\n",
    "    rfc = RandomForestClassifier(n_estimators = 40,\n",
    "     max_depth = 80,\n",
    "     max_features='auto',\n",
    "     min_samples_leaf = 1,\n",
    "     min_samples_split = 4)\n",
    "\n",
    "    #從訓練組資料中建立隨機森林模型\n",
    "    rfc.fit(X_train,y_train)\n",
    "    print(\"score of train\",rfc.score(X_train,y_train))\n",
    "    \n",
    "    #test of rfc\n",
    "    rfc_pred = rfc.predict(X_test)\n",
    "    print(\"score of test :\",rfc.score(X_test, y_test))\n",
    "    print(confusion_matrix(y_test,rfc_pred))\n",
    "    print(classification_report(y_test,rfc_pred))\n",
    "    \n",
    "    #保存Model(注:save資料夾要預先建立，否則會錯誤)\n",
    "    joblib.dump(rfc,FaceModel )\n",
    "    joblib.dump(svc_model,FaceModelSVM )\n",
    "    joblib.dump(svc_model_linear,FaceModelSVMLinear )\n",
    "    joblib.dump(ss,SSModel)\n",
    "    #svc_model = joblib.load('clf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm_linear\n",
      "\n",
      "score of train 0.5684882895670689\n",
      "score of test : 0.5487603305785124\n",
      "[[163 145]\n",
      " [128 169]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.53      0.54       308\n",
      "           1       0.54      0.57      0.55       297\n",
      "\n",
      "    accuracy                           0.55       605\n",
      "   macro avg       0.55      0.55      0.55       605\n",
      "weighted avg       0.55      0.55      0.55       605\n",
      "\n",
      "\n",
      "\n",
      "svm\n",
      "\n",
      "score of train 0.7771469127040455\n",
      "score of test : 0.6479338842975206\n",
      "[[215  93]\n",
      " [120 177]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.70      0.67       308\n",
      "           1       0.66      0.60      0.62       297\n",
      "\n",
      "    accuracy                           0.65       605\n",
      "   macro avg       0.65      0.65      0.65       605\n",
      "weighted avg       0.65      0.65      0.65       605\n",
      "\n",
      "\n",
      "\n",
      "RFC\n",
      "\n",
      "score of train 0.9985805535841022\n",
      "score of test : 0.743801652892562\n",
      "[[231  77]\n",
      " [ 78 219]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75       308\n",
      "           1       0.74      0.74      0.74       297\n",
      "\n",
      "    accuracy                           0.74       605\n",
      "   macro avg       0.74      0.74      0.74       605\n",
      "weighted avg       0.74      0.74      0.74       605\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SvmRfc(df_sita,'save/FaceModel_RFC3.pkl','save/ssFace3.pkl','save/FaceModel_SVM3.pkl','save/FaceModel_SVMLinear3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def modelPredict2(df_sita,df,faceModel,ssModel,clf):    \n",
    "    #載入正臉model\n",
    "    face = joblib.load(faceModel)\n",
    "    ssFace = joblib.load(ssModel)\n",
    "    #輸入sita!!!!!!!\n",
    "    Sita = df_sita\n",
    "    Sitadata = Sita.drop(labels='face',axis=1).values\n",
    "    data = ssFace.transform(Sitadata)\n",
    "\n",
    "    pred = face.predict(data)\n",
    "\n",
    "    df=df\n",
    "    df['face']=pred\n",
    "\n",
    "    df=df[df.face == 1]\n",
    "    dfh=df[df.painlevel ==0]\n",
    "    dfp=df[df.painlevel ==1]\n",
    "    \n",
    "    #Num of pain = Num of health\n",
    "\n",
    "    if len(dfp) >= len(dfh):\n",
    "        dfp=pd.DataFrame.sample(dfp,n=len(dfh),random_state=1).reset_index(drop=True)\n",
    "        print(len(dfp))\n",
    "    elif len(dfp) < len(dfh):\n",
    "        dfh=pd.DataFrame.sample(dfh,n=len(dfp),random_state=1).reset_index(drop=True)\n",
    "        print(len(dfh))\n",
    "\n",
    "    # df of train = health+pain\n",
    "    df = pd.concat([dfh,dfp]).reset_index(drop=True)\n",
    "    df\n",
    "\n",
    "    #正規化 > train&test\n",
    "    data = df.drop(labels=['painlevel','face'],axis=1).values\n",
    "    target = df.painlevel.values\n",
    "\n",
    "    \n",
    "    for j in range(10):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=10/30, random_state=10*(j+1)) \n",
    "        ss = StandardScaler()\n",
    "        X_train = ss.fit_transform(X_train)\n",
    "        X_test = ss.transform(X_test)\n",
    "\n",
    "        \n",
    "\n",
    "        #predict test\n",
    "        predicted = svc.predict(X_test)\n",
    "\n",
    "        print(\"score of test :\",svc.score(X_test, y_test))\n",
    "\n",
    "        print(metrics.classification_report(y_test, predicted))\n",
    "        print(metrics.confusion_matrix(y_test, predicted))\n",
    "\n",
    "        tT = 0\n",
    "        tF = 0\n",
    "        fT = 0\n",
    "        fF = 0\n",
    "        for i in range(len(y_test)):\n",
    "            if (y_test[i] == 0)&(predicted[i] == 0):\n",
    "                tT += 1\n",
    "            elif (y_test[i] == 0)&(predicted[i] == 1):\n",
    "                tF += 1\n",
    "            elif (y_test[i] == 1)&(predicted[i] == 0):\n",
    "                fT += 1\n",
    "            elif (y_test[i] == 1)&(predicted[i] == 1):\n",
    "                fF += 1\n",
    "        print(tT,tF,fT,fF)\n",
    "        \n",
    "        ##save result\n",
    "        file = 'result_NO.csv'\n",
    "        with open(file, 'a+', newline = '') as csvFile:\n",
    "            csvWriter = csv.writer(csvFile)\n",
    "            csvWriter.writerow([tT,tF,fT,fF,clf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm_line\n",
      "666\n",
      "score of test : 0.6509009009009009\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.66      0.62      0.64       220\n",
      "         1.0       0.65      0.68      0.66       224\n",
      "\n",
      "    accuracy                           0.65       444\n",
      "   macro avg       0.65      0.65      0.65       444\n",
      "weighted avg       0.65      0.65      0.65       444\n",
      "\n",
      "[[137  83]\n",
      " [ 72 152]]\n",
      "137 83 72 152\n",
      "score of test : 0.6509009009009009\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.66      0.66       223\n",
      "         1.0       0.65      0.64      0.65       221\n",
      "\n",
      "    accuracy                           0.65       444\n",
      "   macro avg       0.65      0.65      0.65       444\n",
      "weighted avg       0.65      0.65      0.65       444\n",
      "\n",
      "[[148  75]\n",
      " [ 80 141]]\n",
      "148 75 80 141\n",
      "score of test : 0.6936936936936937\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.68      0.67      0.68       212\n",
      "         1.0       0.70      0.72      0.71       232\n",
      "\n",
      "    accuracy                           0.69       444\n",
      "   macro avg       0.69      0.69      0.69       444\n",
      "weighted avg       0.69      0.69      0.69       444\n",
      "\n",
      "[[142  70]\n",
      " [ 66 166]]\n",
      "142 70 66 166\n",
      "score of test : 0.6238738738738738\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.60      0.61      0.61       210\n",
      "         1.0       0.65      0.63      0.64       234\n",
      "\n",
      "    accuracy                           0.62       444\n",
      "   macro avg       0.62      0.62      0.62       444\n",
      "weighted avg       0.62      0.62      0.62       444\n",
      "\n",
      "[[129  81]\n",
      " [ 86 148]]\n",
      "129 81 86 148\n",
      "score of test : 0.6734234234234234\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.64      0.67      0.66       206\n",
      "         1.0       0.70      0.67      0.69       238\n",
      "\n",
      "    accuracy                           0.67       444\n",
      "   macro avg       0.67      0.67      0.67       444\n",
      "weighted avg       0.68      0.67      0.67       444\n",
      "\n",
      "[[139  67]\n",
      " [ 78 160]]\n",
      "139 67 78 160\n",
      "score of test : 0.6734234234234234\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.69      0.65      0.67       226\n",
      "         1.0       0.66      0.70      0.68       218\n",
      "\n",
      "    accuracy                           0.67       444\n",
      "   macro avg       0.67      0.67      0.67       444\n",
      "weighted avg       0.67      0.67      0.67       444\n",
      "\n",
      "[[147  79]\n",
      " [ 66 152]]\n",
      "147 79 66 152\n",
      "score of test : 0.6801801801801802\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.69      0.63      0.66       216\n",
      "         1.0       0.68      0.72      0.70       228\n",
      "\n",
      "    accuracy                           0.68       444\n",
      "   macro avg       0.68      0.68      0.68       444\n",
      "weighted avg       0.68      0.68      0.68       444\n",
      "\n",
      "[[137  79]\n",
      " [ 63 165]]\n",
      "137 79 63 165\n",
      "score of test : 0.6711711711711712\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.70      0.65      0.67       232\n",
      "         1.0       0.64      0.70      0.67       212\n",
      "\n",
      "    accuracy                           0.67       444\n",
      "   macro avg       0.67      0.67      0.67       444\n",
      "weighted avg       0.67      0.67      0.67       444\n",
      "\n",
      "[[150  82]\n",
      " [ 64 148]]\n",
      "150 82 64 148\n",
      "score of test : 0.6644144144144144\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.66      0.65       214\n",
      "         1.0       0.68      0.67      0.67       230\n",
      "\n",
      "    accuracy                           0.66       444\n",
      "   macro avg       0.66      0.66      0.66       444\n",
      "weighted avg       0.66      0.66      0.66       444\n",
      "\n",
      "[[141  73]\n",
      " [ 76 154]]\n",
      "141 73 76 154\n",
      "score of test : 0.6486486486486487\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.61      0.64       226\n",
      "         1.0       0.63      0.69      0.66       218\n",
      "\n",
      "    accuracy                           0.65       444\n",
      "   macro avg       0.65      0.65      0.65       444\n",
      "weighted avg       0.65      0.65      0.65       444\n",
      "\n",
      "[[138  88]\n",
      " [ 68 150]]\n",
      "138 88 68 150\n",
      "svm\n",
      "686\n",
      "score of test : 0.8449781659388647\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.88      0.85       223\n",
      "         1.0       0.88      0.81      0.84       235\n",
      "\n",
      "    accuracy                           0.84       458\n",
      "   macro avg       0.85      0.85      0.84       458\n",
      "weighted avg       0.85      0.84      0.84       458\n",
      "\n",
      "[[196  27]\n",
      " [ 44 191]]\n",
      "196 27 44 191\n",
      "score of test : 0.834061135371179\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.79      0.83       227\n",
      "         1.0       0.81      0.87      0.84       231\n",
      "\n",
      "    accuracy                           0.83       458\n",
      "   macro avg       0.84      0.83      0.83       458\n",
      "weighted avg       0.84      0.83      0.83       458\n",
      "\n",
      "[[180  47]\n",
      " [ 29 202]]\n",
      "180 47 29 202\n",
      "score of test : 0.8406113537117904\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.80      0.83       217\n",
      "         1.0       0.83      0.88      0.85       241\n",
      "\n",
      "    accuracy                           0.84       458\n",
      "   macro avg       0.84      0.84      0.84       458\n",
      "weighted avg       0.84      0.84      0.84       458\n",
      "\n",
      "[[174  43]\n",
      " [ 30 211]]\n",
      "174 43 30 211\n",
      "score of test : 0.8231441048034934\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.83      0.82       221\n",
      "         1.0       0.84      0.81      0.83       237\n",
      "\n",
      "    accuracy                           0.82       458\n",
      "   macro avg       0.82      0.82      0.82       458\n",
      "weighted avg       0.82      0.82      0.82       458\n",
      "\n",
      "[[184  37]\n",
      " [ 44 193]]\n",
      "184 37 44 193\n",
      "score of test : 0.8100436681222707\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.81      0.81       225\n",
      "         1.0       0.81      0.81      0.81       233\n",
      "\n",
      "    accuracy                           0.81       458\n",
      "   macro avg       0.81      0.81      0.81       458\n",
      "weighted avg       0.81      0.81      0.81       458\n",
      "\n",
      "[[182  43]\n",
      " [ 44 189]]\n",
      "182 43 44 189\n",
      "score of test : 0.8253275109170306\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.85      0.83       230\n",
      "         1.0       0.84      0.80      0.82       228\n",
      "\n",
      "    accuracy                           0.83       458\n",
      "   macro avg       0.83      0.83      0.83       458\n",
      "weighted avg       0.83      0.83      0.83       458\n",
      "\n",
      "[[195  35]\n",
      " [ 45 183]]\n",
      "195 35 45 183\n",
      "score of test : 0.8427947598253275\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.84      0.84       224\n",
      "         1.0       0.85      0.85      0.85       234\n",
      "\n",
      "    accuracy                           0.84       458\n",
      "   macro avg       0.84      0.84      0.84       458\n",
      "weighted avg       0.84      0.84      0.84       458\n",
      "\n",
      "[[188  36]\n",
      " [ 36 198]]\n",
      "188 36 36 198\n",
      "score of test : 0.8449781659388647\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.84      0.85       230\n",
      "         1.0       0.84      0.85      0.84       228\n",
      "\n",
      "    accuracy                           0.84       458\n",
      "   macro avg       0.84      0.84      0.84       458\n",
      "weighted avg       0.84      0.84      0.84       458\n",
      "\n",
      "[[194  36]\n",
      " [ 35 193]]\n",
      "194 36 35 193\n",
      "score of test : 0.834061135371179\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.78      0.83       231\n",
      "         1.0       0.80      0.89      0.84       227\n",
      "\n",
      "    accuracy                           0.83       458\n",
      "   macro avg       0.84      0.83      0.83       458\n",
      "weighted avg       0.84      0.83      0.83       458\n",
      "\n",
      "[[181  50]\n",
      " [ 26 201]]\n",
      "181 50 26 201\n",
      "score of test : 0.8209606986899564\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.83      0.83       237\n",
      "         1.0       0.81      0.81      0.81       221\n",
      "\n",
      "    accuracy                           0.82       458\n",
      "   macro avg       0.82      0.82      0.82       458\n",
      "weighted avg       0.82      0.82      0.82       458\n",
      "\n",
      "[[196  41]\n",
      " [ 41 180]]\n",
      "196 41 41 180\n",
      "rfc\n",
      "820\n",
      "score of test : 0.9177330895795247\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.91      0.91       258\n",
      "         1.0       0.92      0.92      0.92       289\n",
      "\n",
      "    accuracy                           0.92       547\n",
      "   macro avg       0.92      0.92      0.92       547\n",
      "weighted avg       0.92      0.92      0.92       547\n",
      "\n",
      "[[235  23]\n",
      " [ 22 267]]\n",
      "235 23 22 267\n",
      "score of test : 0.943327239488117\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.97      0.95       291\n",
      "         1.0       0.96      0.92      0.94       256\n",
      "\n",
      "    accuracy                           0.94       547\n",
      "   macro avg       0.94      0.94      0.94       547\n",
      "weighted avg       0.94      0.94      0.94       547\n",
      "\n",
      "[[281  10]\n",
      " [ 21 235]]\n",
      "281 10 21 235\n",
      "score of test : 0.9360146252285192\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.93      0.93       262\n",
      "         1.0       0.93      0.94      0.94       285\n",
      "\n",
      "    accuracy                           0.94       547\n",
      "   macro avg       0.94      0.94      0.94       547\n",
      "weighted avg       0.94      0.94      0.94       547\n",
      "\n",
      "[[243  19]\n",
      " [ 16 269]]\n",
      "243 19 16 269\n",
      "score of test : 0.9140767824497258\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.90      0.91       279\n",
      "         1.0       0.90      0.93      0.91       268\n",
      "\n",
      "    accuracy                           0.91       547\n",
      "   macro avg       0.91      0.91      0.91       547\n",
      "weighted avg       0.91      0.91      0.91       547\n",
      "\n",
      "[[251  28]\n",
      " [ 19 249]]\n",
      "251 28 19 249\n",
      "score of test : 0.9305301645338209\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.91      0.93       276\n",
      "         1.0       0.91      0.95      0.93       271\n",
      "\n",
      "    accuracy                           0.93       547\n",
      "   macro avg       0.93      0.93      0.93       547\n",
      "weighted avg       0.93      0.93      0.93       547\n",
      "\n",
      "[[252  24]\n",
      " [ 14 257]]\n",
      "252 24 14 257\n",
      "score of test : 0.9195612431444241\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.90      0.92       271\n",
      "         1.0       0.91      0.94      0.92       276\n",
      "\n",
      "    accuracy                           0.92       547\n",
      "   macro avg       0.92      0.92      0.92       547\n",
      "weighted avg       0.92      0.92      0.92       547\n",
      "\n",
      "[[244  27]\n",
      " [ 17 259]]\n",
      "244 27 17 259\n",
      "score of test : 0.9250457038391224\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.91      0.92       276\n",
      "         1.0       0.91      0.94      0.93       271\n",
      "\n",
      "    accuracy                           0.93       547\n",
      "   macro avg       0.93      0.93      0.93       547\n",
      "weighted avg       0.93      0.93      0.93       547\n",
      "\n",
      "[[250  26]\n",
      " [ 15 256]]\n",
      "250 26 15 256\n",
      "score of test : 0.9323583180987203\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.91      0.93       265\n",
      "         1.0       0.92      0.95      0.94       282\n",
      "\n",
      "    accuracy                           0.93       547\n",
      "   macro avg       0.93      0.93      0.93       547\n",
      "weighted avg       0.93      0.93      0.93       547\n",
      "\n",
      "[[241  24]\n",
      " [ 13 269]]\n",
      "241 24 13 269\n",
      "score of test : 0.9579524680073126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.94      0.96       270\n",
      "         1.0       0.94      0.98      0.96       277\n",
      "\n",
      "    accuracy                           0.96       547\n",
      "   macro avg       0.96      0.96      0.96       547\n",
      "weighted avg       0.96      0.96      0.96       547\n",
      "\n",
      "[[253  17]\n",
      " [  6 271]]\n",
      "253 17 6 271\n",
      "score of test : 0.9414990859232175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.95      0.94       275\n",
      "         1.0       0.95      0.93      0.94       272\n",
      "\n",
      "    accuracy                           0.94       547\n",
      "   macro avg       0.94      0.94      0.94       547\n",
      "weighted avg       0.94      0.94      0.94       547\n",
      "\n",
      "[[261  14]\n",
      " [ 18 254]]\n",
      "261 14 18 254\n"
     ]
    }
   ],
   "source": [
    "print('svm_line')\n",
    "modelPredict2(df_sita,df,'save/FaceModel_SVMLinear3.pkl','save/ssFace3.pkl','SVMLinear3')\n",
    "print('svm')\n",
    "modelPredict2(df_sita,df,'save/FaceModel_SVM3.pkl','save/ssFace3.pkl','SVM3')\n",
    "print('rfc')\n",
    "modelPredict2(df_sita,df,'save/FaceModel_RFC3.pkl','save/ssFace3.pkl','RFC3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
